> **Note:** This document describes the **Python implementation** of the HLN standard. For the high-level architectural specification, please refer to the `hln` package.

[EN](https://github.com/avtomatika-ai/avtomatika/blob/main/docs/architecture.md) | [ES](https://github.com/avtomatika-ai/avtomatika/blob/main/docs/es/architecture.md) | **RU**

# Архитектура Оркестратора

Этот документ описывает высокоуровневую архитектуру системы оркестрации, ее ключевые компоненты и их взаимодействие.

Как было представлено в `README.md`, архитектуру можно рассматривать через аналогию **"Режиссёр-Сценарий-Исполнители"**. Этот документ подробно раскрывает техническую реализацию каждого из этих элементов.

## Общая схема

Система состоит из центрального **Оркестратора** и множества **Воркеров**. Все внешние взаимодействия (от клиентов и воркеров) проходят через **NGINX Reverse Proxy**, который обеспечивает поддержку HTTP/2 и терминирует TLS-соединения.

### Схема компонентов
```mermaid
graph TD
    subgraph "Внешний мир"
        Client[API Клиент]
    end

    subgraph "Инфраструктура"
        NGINX(NGINX Reverse Proxy)
    end

    subgraph "Система Оркестрации"
        O_API(Orchestrator API)
        O_Engine{Engine}
        O_Storage[(Storage: Redis)]
        W1(Воркер 1)
        W2(Воркер 2)
    end

    Client --"1. Создает Job (HTTPS/2)"--> NGINX
    NGINX --"2. Проксирует запрос"--> O_API
    O_API --> O_Engine
    O_Engine --> O_Storage

    W1 --"3. Long-Polling для задач (HTTP)"--> NGINX
    NGINX --> O_API
    O_API --"4. Отдает задачу"--> NGINX
    NGINX --> W1

    W1 --"5. Отправляет результат"--> NGINX

    W1 <-."6. Команда 'cancel' (WebSocket)".-> NGINX
    NGINX <-." ".-> O_API

    W1 --"Heartbeat"--> NGINX
    W2 --"Heartbeat"--> NGINX
```

## Ключевые Компоненты Оркестратора

### 0. Слой Протокола (`rxon`)
**Расположение:** `src/avtomatika/protocol/`

Этот слой определяет строгий контракт взаимодействия, гарантируя, что бизнес-логика отделена от деталей транспорта.
-   **Модели:** Структуры данных (`WorkerRegistration`, `TaskResult`), определенные как `NamedTuple` (готовы к миграции на Pydantic/Protobuf).
-   **Безопасность:** Примитивы безопасности, независимые от транспорта (фабрики SSLContext, извлечение Identity).
-   **Константы:** Общие коды статусов и ошибок.

### 0.1 Сервисный Слой (Service Layer)
**Расположение:** `src/avtomatika/services/`

Инкапсулирует основную бизнес-логику, отделяя ее от HTTP API.
-   **`WorkerService`**: Управляет жизненным циклом воркеров, регистрацией, распределением задач, обработкой результатов и выдачей STS токенов.
-   **API Handlers**: Теперь выступают как тонкие обертки, которые парсят HTTP-запросы и делегируют выполнение Сервисам.

### 1. `OrchestratorEngine`
**Расположение:** `src/avtomatika/engine.py`

Это центральный класс, который связывает все компоненты вместе. Его основные задачи:
- Инициализация веб-приложения `aiohttp` и делегирование настройки маршрутов слою API.
- **Инициализация Сервисов:** Запускает `WorkerService`, `S3Service` и др.
- Регистрация "Блупринтов" (`StateMachineBlueprint`).
- Управление жизненным циклом фоновых процессов (`JobExecutor`, `Watcher`, `HealthChecker`, `ReputationCalculator`, `Scheduler`).
- Предоставление доступа к общим ресурсам через `aiohttp.web.AppKey`.

### 1.1. API Layer
**Путь:** `src/avtomatika/api/`

Логика обработки HTTP API была отделена от основного движка для улучшения поддерживаемости.
-   **`routes.py`**: Отвечает за настройку таблицы маршрутизации приложения для Client и Public API.
-   **`handlers.py`**: Содержит непосредственные обработчики запросов для эндпоинтов Client и Public API.
-   **Worker API**: Теперь обрабатывается классом `HttpListener` из библиотеки `rxon`, который интегрирован напрямую в `OrchestratorEngine`. Этот слушатель управляет низкоуровневыми деталями HTTP протокола RXON.

### 2. `StateMachineBlueprint`
**Расположение:** `src/avtomatika/blueprint.py`

Это декларативный способ определения рабочего процесса (пайплайна).
- **Конечный автомат:** Каждый блупринт представляет собой конечный автомат, где состояния - это шаги процесса, а переходы определяются логикой внутри "хендлеров".
- **Хендлеры:** Функции, привязанные к определенным состояниям с помощью декоратора `@blueprint.handler_for("state_name")`. Они получают `JobContext` и `ActionFactory` для выполнения действий.
- **Явное определение состояний:** Декоратор `@blueprint.handler_for` принимает два булевых флага:
    - `is_start=True`: Помечает состояние как **начальное**. Каждый блупринт должен иметь ровно одно такое состояние.
    - `is_end=True`: Помечает состояние как **конечное** (терминальное). Блупринт может иметь несколько таких состояний.
- **Валидация:** При регистрации блупринта в `OrchestratorEngine` автоматически вызывается метод `validate()`, который проверяет, что в блупринте определено ровно одно начальное состояние. Это предотвращает ошибки конфигурации на раннем этапе.
- **Условия:** Поддерживает условные переходы с помощью модификатора `.when("context.area.field == 'value'")`, что позволяет создавать гибкую логику маршрутизации.
- **Визуализация:** Предоставляет метод `.render_graph()` для автоматической генерации диаграммы состояний с помощью `graphviz`, что упрощает анализ и документирование логики пайплайна.
- **Параллельное выполнение и Агрегация:** Позволяет запускать несколько независимых задач одновременно. После их завершения специальный хендлер-**агрегатор** собирает все результаты для дальнейшей обработки.

### 3. `JobExecutor`
**Расположение:** `src/avtomatika/executor.py`

Это основной фоновый процесс, который отвечает за выполнение заданий.
- **Цикл выполнения:** Постоянно извлекает задания из очереди в Redis (`dequeue_job`).
- **Обработка задания:** Для каждого задания он находит соответствующий хендлер в блупринте и выполняет его.
- **Управление состоянием:** После выполнения хендлера он обрабатывает действия, запрошенные через `ActionFactory`. Это может быть простой переход в новое состояние (`transition_to`) или более сложная логика, например, отправка задачи воркеру (`dispatch_task`).

  **Асинхронные переходы с `dispatch_task`**

  Одной из ключевых возможностей является управление процессом, который зависит от результата асинхронной задачи, выполненной воркером. Это реализуется с помощью параметра `transitions` в методе `dispatch_task`.

  ```python
  # Пример хендлера
  async def generate_data_handler(context, actions):
      actions.dispatch_task(
          task_type="data_generation",
          params=context.initial_data,
          transitions={
              "success": "awaiting_approval",
              "failure": "data_generation_failed",
              "needs_review": "manual_review_step"
          }
      )
  ```

  В этом примере:
  1. Оркестратор отправляет задачу воркеру и сохраняет словарь `transitions` в состоянии `Job`.
  2. `Job` переходит в состояние ожидания (`waiting_for_worker`).
  3. Воркер выполняет задачу и возвращает результат, содержащий статус (например, `"success"`, `"failure"` или кастомный статус `"needs_review"`).
  4. Оркестратор использует этот статус как ключ в сохраненном словаре `transitions` для определения следующего состояния.

  **Важные правила поведения:**
  - **Статус по умолчанию:** Если воркер не возвращает статус, система по умолчанию считает его `"success"`.
  - **Передача данных от Воркера:** Помимо `status`, воркер может вернуть объект `data`. Содержимое этого объекта (если это словарь) будет автоматически скопировано в `context.state_history` и станет доступно на следующих шагах выполнения блупринта.
  - **Неизвестный статус:** Если воркер возвращает статус, для которого нет ключа в словаре `transitions`, процесс автоматически переходит в состояние с именем `"failed"`. Это обеспечивает отказоустойчивость и предотвращает "зависание" процессов.

- **Отказоустойчивость:** Реализует логику повторных попыток (retry) и перемещения сбойных задач в карантин. Оркестратор обрабатывает ошибки от воркеров на основе их типа, что позволяет гибко управлять сбойными задачами.

  **Типы ошибок, возвращаемые воркером:**
  Воркер может вернуть один из трех типов ошибок в поле `error.code`:
  - `TRANSIENT_ERROR` (По умолчанию): Указывает на временную проблему (например, сбой сети, недоступность внешнего сервиса). Оркестратор будет **повторять** отправку задачи несколько раз, прежде чем переместить ее в карантин.
  - `PERMANENT_ERROR`: Указывает на постоянную проблему, которая не решится повторной попыткой (например, поврежденный файл, несовместимая версия). Задача будет немедленно **перемещена в карантин** без повторных попыток.
  - `INVALID_INPUT_ERROR`: Указывает на фундаментальную проблему с входными данными задачи (неверный формат, отсутствующие параметры). `Job`, к которому относится задача, будет немедленно **переведен в состояние `failed`**, так как дальнейшее выполнение бессмысленно.

  **Уровни отказоустойчивости:**
  - **Повтор на уровне задачи:** Оркестратор управляет повторными попытками для `TRANSIENT_ERROR`.
  - **Повтор на уровне Job-а:** Если возникает исключение в коде самого хендлера (в Оркестраторе), `JobExecutor` повторит выполнение этого же хендлера несколько раз, прежде чем поместить весь `Job` в карантин.

  **Параллельное выполнение задач**

  Система позволяет запускать несколько задач параллельно и затем собирать их результаты. Это достигается за счет специального использования метода `dispatch_task` и хендлеров-агрегаторов.

  1.  **Запуск параллельных веток:**
      - В одном из хендлеров вы можете вызвать `actions.dispatch_task` несколько раз, каждый раз для разной задачи.
      - **Ключевой момент:** Все эти вызовы `dispatch_task` должны иметь один и тот же следующий шаг (`transition`), который ведет к хендлеру-агрегатору.

      ```python
      # Хендлер, запускающий параллельные задачи
      @blueprint.handler_for("start_parallel_tasks")
      async def start_parallel_work(context, actions):
          # Запускаем задачу A
          actions.dispatch_task(
              task_type="task_a",
              params={"id": 1},
              transitions={"success": "aggregate_results"}
          )
          # Запускаем задачу B
          actions.dispatch_task(
              task_type="task_b",
              params={"id": 2},
              transitions={"success": "aggregate_results"}
          )
      ```
  2.  **Агрегация результатов:**
      - Хендлер, предназначенный для сбора результатов, помечается специальным декоратором `@blueprint.aggregator_for("state_name")`.
      - Этот хендлер будет выполнен только **после того, как все параллельные ветки, ведущие в это состояние, завершатся**.
      - Внутри агрегатора результаты всех выполненных задач доступны через `context.aggregation_results`.

      ```python
      # Хендлер-агрегатор
      @blueprint.aggregator_for("aggregate_results")
      async def aggregator(context, actions):
          # Результаты доступны в виде словаря: {task_id: result_dict}
          results = context.aggregation_results

          summary = {}
          for task_id, result in results.items():
              summary[task_id] = result.get("data")

          # Сохраняем итоговый результат и переходим дальше
          context.state_history["summary"] = summary
          actions.transition_to("final_step")
      ```
      - `context.aggregation_results` — это словарь, где ключами являются ID задач, а значениями — полные объекты результатов, возвращенные воркерами.

### 3.1. `JobContext` (Объект Контекста)
Каждый хендler получает на вход объект `context`, который содержит всю необходимую информацию о текущем задании и предоставляет доступ к ресурсам. Это основной способ получения данных внутри пайплайна.

**Ключевые атрибуты:**
- `context.job_id` (str): Уникальный идентификатор текущего задания.
- `context.current_state` (str): Имя текущего состояния, в котором выполняется хендлер.
- `context.initial_data` (dict): Исходные данные, с которыми было создано задание.
- `context.state_history` (dict): Словарь, в котором можно хранить и читать данные, персистентные в рамках одного `Job`. Полезен для передачи информации между состояниями.
- `context.aggregation_results` (dict): Доступен **только в хендлерах-агрегаторах**. Содержит результаты всех параллельных задач, которые выполнялись перед агрегацией. Ключи — `task_id`, значения — словари с результатами.
- `context.client` (ClientConfig): Объект, содержащий информацию о клиенте, который инициировал задание (включая его план и кастомные параметры из `clients.toml`).
- `context.data_stores` (namespace): Объект, предоставляющий доступ ко всем `data_store`, привязанным к блупринту. Доступ осуществляется по имени, например: `context.data_stores.my_cache.get("key")`.
- `context.tracing_context` (dict): Словарь с информацией для распределенной трассировки OpenTelemetry.

### 3.2. `ActionFactory` (Фабрика Действий)
Вторым аргументом каждый хендлер получает объект `actions`. Этот объект предоставляет методы, с помощью которых хендлер определяет, что должно произойти после его выполнения. **В рамках одного хендлера можно вызвать только один метод `actions`**.

**Ключевые методы:**
- `actions.transition_to(state: str)`: Просто переводит конечный автомат в следующее состояние. Выполнение продолжится немедленно.
- `actions.dispatch_task(task_type, params, transitions, priority, ...)`: Отправляет задачу на выполнение воркеру и приостанавливает пайплайн до получения результата. Словарь `transitions` определяет, в какое состояние перейдет пайплайн в зависимости от `status`, возвращенного воркером. Параметр `priority` позволяет указать приоритет задачи.
- `actions.await_human_approval(message, transitions)`: Приостанавливает пайплайн до тех пор, пока внешняя система или человек не отправит вебхук с решением.
- `actions.run_blueprint(blueprint_name, initial_data, transitions)`: Запускает другой (дочерний) блупринт как часть текущего пайплайна. Основной пайплайн будет приостановлен до завершения дочернего.

### 3.3. Внедрение Зависимостей (`DataStore`)
Система поддерживает механизм внедрения зависимостей, который позволяет предоставлять хендлерам доступ к внешним ресурсам, таким как клиенты баз данных, API-клиенты или кэши.

- **Регистрация:** Любой объект (например, инстанс клиента `redis.Redis` или кастомного API-клиента) может быть зарегистрирован как `DataStore` при создании блупринта.
  ```python
  # Пример регистрации
  from .datastore import AsyncDictStore

  my_cache = AsyncDictStore({"initial_key": "initial_value"})

  bp = StateMachineBlueprint(
      "my_blueprint_with_datastore",
      data_stores={"cache": my_cache}
  )
  ```
- **Доступ в хендлере:** Зарегистрированные `DataStore` становятся доступны внутри любого хендлера этого блупринта через объект `context`. Доступ осуществляется по имени, которое было указано при регистрации.
  ```python
  @bp.handler_for("some_state")
  async def my_handler(context, actions):
      # Получаем доступ к нашему кэшу
      value = await context.data_stores.cache.get("some_key")
      print(f"Value from cache: {value}")
  ```

Этот механизм позволяет отделить логику бизнес-процесса от конкретных реализаций клиентов для доступа к данным, что упрощает тестирование и переиспользование компонентов.

### 4. `Dispatcher`
**Расположение:** `src/avtomatika/dispatcher.py`

Отвечает за постановку задачи (`task`) в очередь для наиболее подходящего воркера.
- **O(1) Поиск Воркера:** Использует пересечение множеств **Redis Sets** (`SINTER`) для мгновенного поиска свободных воркеров, поддерживающих нужный тип задачи. Это гарантирует константное время поиска даже при тысячах зарегистрированных воркеров.
- **Фильтрация ресурсов:** Применяет дополнительную проверку требований к "железу" (GPU модель, объем VRAM, наличие ML-моделей) к найденному списку кандидатов.
- **Стратегии:** К отфильтрованному пулу воркеров применяет одну из стратегий выбора:
    - `default`: Предпочитает "прогретых" воркеров (у которых уже есть нужные модели в памяти), а затем из них выбирает самый дешевый.
    - `round_robin`: Распределяет нагрузку поочередно между всеми доступными воркерами.
    - `least_connections`: Выбирает воркера с наименьшим количеством активных задач.
    - `cheapest`: Выбирает воркер с наименьшей стоимостью работы в секунду (`cost_per_second`).
    - `best_value`: Выбирает воркер с наилучшим соотношением "цена/качество", используя его **репутацию**.
- **Постановка в очередь:** Помещает задачу в персональную приоритетную очередь выбранного воркера в `Storage`.

### 4.1. Взаимодействие с Воркерами (Pull-модель)

Система использует **Pull-модель**, при которой воркеры сами инициируют соединение с оркестратором для получения задач. Это позволяет воркерам находиться за NAT или файрволами.

- **Регистрация и Heartbeats:** Воркер при запуске регистрируется в оркестраторе, сообщая о своих возможностях. Затем он периодически отправляет heartbeat-сообщения для подтверждения своей активности. Этот механизм остался прежним.
- **Получение задач (Long-Polling):**
    1. Воркер отправляет `GET` запрос на эндпоинт `/_worker/workers/{worker_id}/tasks/next`.
    2. Если для этого воркера уже есть задача в очереди (в Redis), оркестратор немедленно возвращает ее в ответе `200 OK`.
    3. Если задач нет, оркестратор удерживает соединение открытым (long-polling) в течение определенного таймаута (например, 30 секунд).
    4. Если за это время `Dispatcher` помещает задачу в очередь для этого воркера, она сразу же отправляется ожидающему воркеру.
    5. Если таймаут истекает, а задач так и не появилось, оркестратор отвечает `204 No Content`, и воркер немедленно делает новый запрос.
- **Пассивный HealthChecker:** Компонент `HealthChecker` остается пассивным. "Здоровье" воркеров определяется по наличию актуальных heartbeat-сообщений в Redis (через механизм TTL). Если воркер перестает их отправлять, он автоматически считается неактивным и исключается из диспетчеризации.

#### **Отказоустойчивость и балансировка нагрузки на стороне Воркера**

Для повышения надежности и распределения нагрузки `worker_sdk` поддерживает подключение к нескольким Оркестраторам. Это поведение настраивается на стороне воркера и прозрачно для Оркестратора.

-   **Конфигурация:** Воркеру передается список адресов Оркестраторов через переменную окружения `ORCHESTRATORS_CONFIG`.
-   **Режимы работы (`MULTI_ORCHESTRATOR_MODE`):**
    -   `FAILOVER` (режим по умолчанию): Воркер будет постоянно опрашивать первый Оркестратор в списке. Если запрос не удается (например, из-за сбоя сети или недоступности сервера), он немедленно переключается на следующий Оркестратор в списке и продолжает опрашивать его. Это обеспечивает высокую доступность.
    -   `ROUND_ROBIN`: Воркер отправляет запросы на получение задач поочередно к каждому Оркестратору из списка. Этот режим полезен для горизонтального масштабирования и распределения нагрузки между несколькими независимыми инстансами Оркестратора.
-   **Регистрация:** При запуске воркер регистрируется и отправляет heartbeat-сообщения *всем* Оркестраторам из списка, чтобы каждый из них был осведомлен о его состоянии и возможностях.

### 4.2. Продвинутая коммуникация: WebSocket

В дополнение к Pull-модели для получения задач, система поддерживает опциональный **двунаправленный канал связи через WebSocket**.

- **Инициация:** Воркер, если он поддерживает эту функцию (и она включена через `WORKER_ENABLE_WEBSOCKETS=true`), устанавливает постоянное WebSocket-соединение с Оркестратором.
- **Назначение:**
    - **Команды от Оркестратора:** Оркестратор может отправлять команды воркеру в реальном времени. Основной пример — команда на отмену выполняемой задачи (`cancel_task`).
    - **Обновления от Воркера:** Воркер может использовать тот же канал для отправки промежуточных обновлений о ходе выполнения задачи (`progress_update`).
- **Отказоустойчивость:** SDK воркера автоматически управляет переподключением в случае обрыва связи.

Эта гибридная модель (HTTP для получения задач, WebSocket для команд и обновлений) позволяет сочетать надежность и простоту Pull-модели с интерактивностью Push-уведомлений.

### 5. `Watcher`
**Расположение:** `src/avtomatika/watcher.py`

Фоновый процесс, который следит за "зависшими" или просроченными задачами.
- **Отслеживание:** Проверяет отсортированный список в Redis, содержащий `job_id` и время их таймаута.
- **Обработка таймаутов:** Если задача превысила таймаут, `Watcher` переводит ее в состояние `failed`, чтобы предотвратить блокировку системы.

### 6. `ReputationCalculator`
**Расположение:** `src/avtomatika/reputation.py`

Это фоновый процесс, который отвечает за анализ производительности воркеров и расчет их репутации.
- **Анализ истории:** Периодически запрашивает из `HistoryStorage` историю задач, выполненных каждым воркером.
- **Расчет репутации:** Вычисляет оценку от 0 до 1 на основе успешности задач.
- **Обновление состояния:** Сохраняет новую репутацию в `StorageBackend`.
- **Throttling:** Использует задержки между запросами, чтобы не перегружать базу данных.

### 7. `HealthChecker`
**Расположение:** `src/avtomatika/health_checker.py`

Процесс активного обслуживания системы.
- **Очистка индексов:** Периодически сканирует индексы Redis на предмет "мертвых" ID воркеров (у которых истек TTL основного ключа) и удаляет их. Это предотвращает раздувание индексов и гарантирует эффективность O(1) диспетчера при длительной работе.
- **Распределенная блокировка:** Гарантирует, что очистку выполняет только один экземпляр оркестратора.

### 8. `Scheduler`
**Расположение:** `src/avtomatika/scheduler.py`

Фоновый процесс, запускающий задачи по расписанию, определенному в `schedules.toml`.
-   **Триггеры:** Поддерживает интервальные (каждые N секунд), ежедневные, еженедельные и ежемесячные запуски.
-   **Часовые пояса:** Учитывает глобально настроенный часовой пояс (`TZ`).
-   **Распределенная блокировка:** Использует атомарные блокировки в `StorageBackend` (`set_nx_ttl`), чтобы гарантировать, что запланированная задача запустится ровно один раз среди всех экземпляров Оркестратора.
-   **Интеграция:** Создает задачи напрямую через `OrchestratorEngine.create_background_job`, минуя HTTP API, но записывая события создания в историю.

### 9. `StorageBackend`
**Расположение:** `src/avtomatika/storage/`

Абстракция для хранения всех **текущих** состояний заданий, воркеров и очередей.

- **Реализации:**
    -   **`MemoryStorage` (по умолчанию):** Реализация, хранящая все данные в памяти. Является реализацией по умолчанию, если не настроено другое хранилище. Позволяет запустить приложение без внешних зависимостей, но все состояния теряются при перезапуске.
    -   **`RedisStorage` (рекомендуется для production):** Реализация, использующая Redis для персистентного хранения. Активируется, когда в конфигурации указан `REDIS_HOST`.
        -   **Хранение состояния:** Использует `msgpack` для эффективной бинарной сериализации состояний заданий и воркеров.
        -   **Очереди задач:** Использует **Redis Streams** (Consumer Groups) для обеспечения надежной доставки задач (At-least-once). Поддерживает восстановление необработанных сообщений после перезапуска с помощью `INSTANCE_ID`.
- **Интерфейс:** `storage/base.py` определяет методы, которые должны быть реализованы в любой реализации хранилища.

### 9.1. `HistoryStorage`
**Расположение:** `src/avtomatika/history/`

Это опциональный компонент, отвечающий за запись **истории** выполнения задач для последующего анализа и отладки.

- **Асинхронная буферизация:** Использует внутреннюю очередь `asyncio.Queue` и фоновый процесс для записи событий. Это изолирует основной цикл Оркестратора от задержек базы данных, обеспечивая высокую пропускную способность.
- **Поддерживаемые БД:**
    - **SQLite:** если URI начинается с `sqlite:` (например, `sqlite:history.db`).
        - **Хранение времени:** Использует Unix timestamp (REAL) в UTC для корректной сортировки.
        - **Часовые пояса:** Автоматически конвертирует время в глобально настроенный `TZ` при чтении.
    - **PostgreSQL:** если URI начинается с `postgresql:` (например, `postgresql://user:pass@host/db`).
        - **Хранение времени:** Использует нативный `TIMESTAMPTZ`.
        - **JSON:** Явно сериализует/десериализует JSONB поля для совместимости.
- **Логируемые события:** `state_started`, `state_finished`, `state_failed`, `task_dispatched`.
- **Отказоустойчивость:** Ошибки при записи в `HistoryStorage` не прерывают основной процесс выполнения задачи. Ошибка логируется, но задача продолжает выполняться.
- **Доступ к данным:** История доступна через API эндпоинт `GET /api/jobs/{job_id}/history`.

### 9.2. Доступ к графу Блупринта

Для целей визуализации и отладки, система предоставляет эндпоинт для получения структуры любого зарегистрированного блупринта в виде графа.

-   **Эндпоинт:** `GET /api/v1/blueprints/{blueprint_name}/graph`
-   **Описание:** Возвращает текстовое представление графа указанного блупринта в формате DOT (graph description language). Этот формат может быть отрендерен в изображение с помощью утилиты Graphviz или совместимых JavaScript-библиотек (например, `viz.js`).
-   **Параметры пути:**
    -   `blueprint_name` (string): Имя блупринта, граф которого нужно получить.
-   **Успешный ответ (200 OK):**
    -   **Content-Type:** `text/vnd.graphviz`
    -   **Тело ответа:** Строка в формате DOT, описывающая граф.
-   **Возможные ошибки:**
    -   `404 Not Found`: Если блупринт с указанным именем не найден.
    -   `501 Not Implemented`: Если на сервере оркестратора не установлена утилита `graphviz`, необходимая для генерации графа.

## 10. Аутентификация и Авторизация

Система использует двухуровневую обработку входящих запросов: сначала загрузка конфигурации, а затем применение цепочки промежуточных обработчиков (`middlewares`) для аутентификации и авторизации.

### 10.1. Загрузка конфигурации клиентов
**Расположение:** `src/avtomatika/client_config_loader.py`

При запуске приложения специальный модуль `client_config_loader` выполняет следующие действия:
1.  **Чтение `clients.toml`:** Считывает конфигурационный файл `clients.toml`, содержащий информацию обо всех разрешенных клиентах.
2.  **Загрузка в Redis:** Для каждого клиента в файле:
    -   Сохраняет его статическую конфигурацию (план, лимиты, кастомные параметры) в Redis, используя токен клиента как ключ.
    -   Если для клиента определена квота (`monthly_attempts`), инициализирует соответствующий счетчик в Redis.

Этот процесс выполняется один раз при старте, что позволяет избежать чтения файла при каждом запросе и обеспечивает высокую производительность.

### 10.2. Промежуточные обработчики (Middlewares)

Все запросы к защищенным эндпоинтам проходят через цепочку `middlewares`.

#### **Аутентификация клиентов (`client_auth_middleware`)**
- **Задача:** Проверить, что клиент имеет право доступа к системе.
- **Механизм:**
    1.  Извлекает токен из заголовка `X-Client-Token`.
    2.  Ищет конфигурацию по этому токену в Redis.
    3.  Если конфигурация найдена, она прикрепляется к объекту `request` (в `request["client_config"]`) для использования следующими middlewares в цепочке.
    4.  Если токен невалиден, запрос отклоняется со статусом `401 Unauthorized`.

#### **Аутентификация воркеров (`worker_auth_middleware`)**
- **Задача:** Проверить, что запрос исходит от аутентифицированного воркера.
- **Механизм (Гибридная модель):**
    1.  Извлекает `worker_id` из пути запроса (или из тела для эндпоинта регистрации).
    2.  Извлекает токен из заголовка `X-Worker-Token`.
    3.  **Проверка индивидуального токена (Приоритет):** Middleware ищет в Redis токен, привязанный к конкретному `worker_id`. Если такой токен найден, он сравнивается с предоставленным. В случае несовпадения запрос немедленно отклоняется (без перехода к проверке глобального токена).
    4.  **Проверка глобального токена (Fallback):** Если индивидуальный токен для `worker_id` не найден, middleware проверяет предоставленный токен на совпадение с общим токеном `WORKER_TOKEN` из конфигурации Оркестратора. Это обеспечивает обратную совместимость.
    5.  Если обе проверки не прошли, запрос отклоняется со статусом `401 Unauthorized`.
- **Конфигурация:** Индивидуальные токены задаются в файле `workers.toml` и загружаются в Redis при старте Оркестратора.

#### **Проверка квот (`quota_middleware`)**
- **Задача:** Убедиться, что клиент не превысил свой лимит запросов.
- **Механизм:**
    1.  Работает **после** `client_auth_middleware`.
    2.  Использует `token` из прикрепленной конфигурации клиента (`request["client_config"]`).
    3.  Выполняет атомарную операцию `check_and_decrement_quota` в Redis.
    4.  Если квота исчерпана, запрос отклоняется со статусом `429 Too Many Requests`.

#### **Ограничение частоты запросов (`ratelimit_middleware`)**
- **Задача:** Защитить систему от слишком частых запросов от одного источника (DDoS-атаки, "шумные" клиенты).
- **Механизм:**
    1.  Использует IP-адрес клиента или `worker_id` в качестве ключа для отслеживания.
    2.  Для каждого запроса атомарно увеличивает счетчик в Redis с заданным временем жизни (например, 5 запросов в минуту).
    3.  Если счетчик превышает лимит, запрос отклоняется со статусом `429 Too Many Requests`.

#### **Сжатие ответов (`compression_middleware`)**
- **Задача:** Уменьшить размер тела ответа для экономии трафика.
- **Механизм:**
    1.  Проверяет заголовок `Accept-Encoding` в запросе клиента.
    2.  Если клиент поддерживает `zstd` (предпочтительно) или `gzip`, тело ответа сжимается перед отправкой.
    3.  В ответ добавляется заголовок `Content-Encoding` с указанием использованного алгоритма.

## 11. Горизонтальное Масштабирование (High Availability)

Архитектура системы позволяет запускать несколько экземпляров Оркестратора параллельно для обеспечения отказоустойчивости и распределения нагрузки.

-   **Stateless API:** HTTP API полностью не хранит состояния в памяти процесса (stateless). Все данные о задачах и воркерах хранятся в Redis. Это позволяет балансировщику нагрузки (например, NGINX) распределять запросы между любым количеством инстансов Оркестратора.
-   **Распределенные блокировки (Distributed Locking):** Фоновые процессы, такие как `Watcher` (проверка таймаутов) и `ReputationCalculator` (расчет репутации), используют механизм распределенных блокировок, чтобы гарантировать, что работа выполняется только одним экземпляром в любой момент времени.
    -   **Механизм:** Используется атомарная операция Redis `SET key value NX PX ttl`.
    -   **Поведение:** При запуске цикла фоновая задача пытается захватить глобальный лок. Если лок уже занят другим активным инстансом, текущий инстанс пропускает итерацию. Это предотвращает гонки (race conditions) и дублирование операций.

## 12. Логирование и Наблюдаемость

Avtomatika спроектирована для работы в современных облачных средах с глубоким уровнем наблюдаемости.

- **Структурированное JSON-логирование:** По умолчанию система использует `python-json-logger` для вывода логов в формате JSON, готовых для ELK, Loki или других агрегаторов.
- **Учет часовых поясов:** Все временные метки в логах, API истории и триггерах планировщика учитывают глобальную настройку `TZ`.
- **Трассируемость:** Каждая запись лога, созданная в процессе выполнения задания, автоматически включает `job_id`, а также `worker_id` и `task_id`, где это применимо.
- **Глобальный Rate Limiting:** Защита на базе Redis охватывает все точки входа (Public, Client и Worker API). Используются контекстные лимиты (например, выше для heartbeats, ниже для polling) для обеспечения стабильности.

## 13. Рекомендации по развертыванию и масштабированию

Для обеспечения надежной и производительной работы системы в продакшн-среде рекомендуется следовать следующим практикам:

- **Reverse Proxy и HTTP/2 (Рекомендуемый подход):** Для максимальной производительности и безопасности развертывайте Оркестратор за Reverse Proxy, таким как NGINX. Это дает два ключевых преимущества:
    1.  **Поддержка HTTP/2:** Прокси-сервер берет на себя терминирование соединений по протоколу HTTP/2, который значительно снижает задержки и накладные расходы на сеть за счет мультиплексирования запросов. Это особенно эффективно для частых heartbeat-сообщений от воркеров.
    2.  **Безопасность и масштабирование:** Оркестратор оказывается изолированным от прямого доступа извне. Прокси-сервер также может выполнять роль балансировщика нагрузки (L7), распределяя запросы между несколькими экземплярами Оркестратора, и управлять TLS-терминированием (HTTPS).

    В репозитории представлен готовый пример развертывания с использованием NGINX и Docker Compose, который является рекомендуемым способом запуска системы.

- **Строгая валидация конфигурации:** Система реализует принцип **Fail Fast**. При старте Оркестратор проверяет валидность конфигурационных файлов (`clients.toml`, `workers.toml`). Если найдены ошибки (невалидный TOML, отсутствие обязательных полей), приложение **не запустится**, выбросив исключение. Это предотвращает работу системы в некорректном или небезопасном состоянии.

- **Модернизация транспорта (WebSocket):** Для задач, требующих обмена данными в реальном времени (например, отмена задачи или отправка промежуточного прогресса), система поддерживает опциональный WebSocket-канал. Соединение также должно устанавливаться через Reverse Proxy.

- **Сжатие ответов:** Сжатие реализуется на уровне приложения с помощью кастомного middleware, которое поддерживает **Zstandard** (предпочтительно) и **Gzip** (в качестве фолбэка). Включать дополнительное сжатие на уровне балансировщика не требуется.

- **Ограничение частоты запросов (Rate Limiting):** Рекомендуется настроить базовое ограничение на балансировщике. В дополнение к этому, приложение реализует собственный, более гранулярный механизм rate-limiting для эндпоинтов воркеров (например, для heartbeat-сообщений), который ограничивает частоту запросов для каждого `worker_id`.

- **Алертинг (Alerting):** Система поставляется с набором рекомендованных правил для алертинга в Prometheus. Эти правила помогают отслеживать критические события, такие как отсутствие активных воркеров или высокий процент сбоев в заданиях.

## 13. Payload Offloading (Выгрузка полезной нагрузки)

Для обработки задач, требующих передачи больших объемов данных (видео, датасеты), система использует механизм выгрузки данных в S3, чтобы не перегружать Redis.

- **Технология**: Построена на базе **`obstore`** (Python-биндинги для Rust библиотеки `object_store`), что обеспечивает высокую производительность и асинхронный доступ.
- **Безопасность памяти (Streaming)**:
    - Загрузка и скачивание файлов полностью потоковые.
    - Данные передаются напрямую между сетевым сокетом и диском. Потребление оперативной памяти остается низким и постоянным (O(1)) вне зависимости от размера файла.
    - Это предотвращает ошибки OOM (Out-Of-Memory) даже при работе с файлами в несколько гигабайт.
- **Контроль конкурентности**:
    - Для защиты системы от исчерпания файловых дескрипторов под нагрузкой, все операции с S3 ограничены глобальным семафором (`S3_MAX_CONCURRENCY`).
- **Механизм**:
    1.  **Изоляция**: Файлы каждой задачи хранятся в S3 с префиксом `jobs/{job_id}/`.
    2.  **Жизненный цикл**:
        - Объект `TaskFiles` предоставляет методы (`download`, `upload`, `read_json`), которые автоматически работают с нужными путями.
        - **Автоматическая очистка**: Когда задача переходит в терминальное состояние (`finished`, `failed`), Оркестратор удаляет папку задачи из S3 и локальные временные файлы.

## 14. Наблюдаемость (Observability)

**Примечание:** Функции наблюдаемости (метрики и трассировка) являются опциональными. Для их активации необходимо установить проект с дополнительной зависимостью: `pip install .[telemetry]`. Если зависимости не установлены, система будет работать в обычном режиме, но без сбора и предоставления данных телеметрии.

Система построена с использованием стандарта **OpenTelemetry**, что обеспечивает единый подход к сбору данных о ее работе.

### 14.1. Метрики (Metrics)
- **Технология:** Метрики собираются с помощью `OpenTelemetry SDK` и предоставляются для сбора системой Prometheus через конечную точку `/_public/metrics`.
- **Реализация:**
    - **Инструменты:** Определяются в `packages/avtomatika/src/avtomatika/metrics.py` с использованием стандартных инструментов OpenTelemetry (`Counter`, `Histogram`, `ObservableGauge`).
    - **Экспортер:** `OrchestratorEngine` инициализирует `PrometheusMetricReader`, который автоматически регистрируется в `prometheus-client`.
    - **Колбэки для Gauge:** Для метрик, которые нельзя обновлять инкрементально (например, длина очереди), используется механизм `ObservableGauge`. `OrchestratorEngine` запускает фоновую задачу, которая периодически опрашивает хранилище и кэширует значения. Синхронные колбэки метрик затем мгновенно отдают эти кэшированные значения, что предотвращает блокировку при сборе метрик.

### 14.2. Распределенная трассировка (Distributed Tracing)
- **Технология:** Трассировка также основана на `OpenTelemetry`.
- **Реализация:**
    - **Инструментация:** Клиент `aiohttp`, используемый для связи с воркерами, автоматически инструментирован с помощью `AioHttpClientInstrumentor`.
    - **Распространение контекста:** Оркестратор автоматически передает контекст трассировки (trace context) воркеру в HTTP-заголовках.
    - **Экспорт:** Настроен `OTLP Exporter`, который позволяет отправлять данные трассировки в любой совместимый коллектор (например, OpenTelemetry Collector).
    - **Визуализация:** Собранные трассы могут быть визуализированы в таких системах, как Jaeger или Zipkin. Это позволяет увидеть полный жизненный цикл задания в виде диаграммы Ганта, анализировать задержки и находить узкие места в распределенной системе.
